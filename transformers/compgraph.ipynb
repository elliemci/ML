{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conventional-wealth",
   "metadata": {
    "id": "acute-north"
   },
   "source": [
    "# Notebook Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-alexandria",
   "metadata": {
    "id": "composite-poker"
   },
   "source": [
    "1. Computing backpropagation by hand\n",
    "2. Computing Matrix mulitplications by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-catalyst",
   "metadata": {
    "id": "missing-coupon"
   },
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-hardwood",
   "metadata": {
    "id": "complete-receptor"
   },
   "source": [
    "Most deep learning frameworks provide an automatic differentiation procedure to compute gradients\n",
    "based on the backpropagation algorithm. In these frameworks, all computations\n",
    "are represented as a graph and therefore also the gradient computation becomes a graph.\n",
    "Therefore, graphs in computation and backpropagation are both crucial deep learning fundamentals for the training of neural networks. \n",
    "\n",
    "Mathematical statements can be represented using computational graphs. This is comparable to a descriptive language that provides a functional description of the necessary computation in the context of deep learning models.\n",
    "\n",
    "\n",
    "Below you find an example of a simple network depicted as a computational graph.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-manitoba",
   "metadata": {
    "id": "optional-belfast"
   },
   "source": [
    "From the chain rule we have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial h_{1}}=\\frac{\\partial l}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}}\\left(=\\frac{\\partial l}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}}\\right)\n",
    "$$\n",
    "\n",
    "which can be represented as a computational graph as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-corps",
   "metadata": {
    "id": "harmful-church"
   },
   "source": [
    "![graph.jpg](images/graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-young",
   "metadata": {
    "id": "accomplished-combining"
   },
   "source": [
    "**Note:** Gradient computations are shown in black, application of the chain rule in green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-rebecca",
   "metadata": {
    "id": "beneficial-negative"
   },
   "source": [
    "Hence, we will use a computational graph to compute the derivatives of a function, for **task 1**. And since we are using a Transformer model, which uses matrix mulitplication for computing the different weight matrices, we will look into matrix multiplication in the **second task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-champagne",
   "metadata": {
    "id": "sudden-seeker"
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-shepherd",
   "metadata": {
    "id": "convertible-category"
   },
   "source": [
    "The chain rule with __Newton__ notation: \n",
    "\n",
    "$(f \\circ g)^{\\prime}(x)=f^{\\prime}(g(x)) \\cdot g^{\\prime}(x)$ <br> <br>\n",
    "\n",
    "The chain rule with __Leibniz__ notation: \n",
    "\n",
    "$\\frac{d z}{d x}=\\frac{d z}{d y} \\cdot \\frac{d y}{d x}$\n",
    "\n",
    "We will use the Leibniz notation for our computations. \n",
    "\n",
    "For instance, if we want to compute the derivative of $(2 x+1)^{7}$, we would do the following:\n",
    "\n",
    "$\\begin{aligned} \\frac{d}{d x}(2 x+1)^{7} &=\\left[\\frac{d}{d(2 x+1)}(2 x+1)^{7}\\right] \\frac{d(2 x+1)}{d x} \\\\ &=7(2 x+1)^{6} \\cdot 2 \\\\ &=14(2 x+1)^{6} \\end{aligned}$\n",
    "\n",
    "So, basically, with the chain rule we must first take the derivative of the outer function keeping the inside function untouched. Then we multiply by the derivative of the inside function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-delhi",
   "metadata": {
    "id": "southern-disco"
   },
   "source": [
    "Now let's look how we can actually compute the backpropagation with a computational graph. The compuations consists of the following two computations:\n",
    "\n",
    "- Forward computation\n",
    "- Backward computation\n",
    "\n",
    "The process for determining the value of the mathematical expression represented by computational graphs is known as the __forward pass__. Forward pass refers to the process of sending data from variables from the left (input) to the right (output) in a forward manner.\n",
    "\n",
    "Let us consider a concrete example to demonstrate its working:\n",
    "\n",
    "$f(x, y, z)=(x+y) z$ <br>\n",
    "e.g. $\\mathrm{x}=-2, \\mathrm{y}=5, \\mathrm{z}=-4$\n",
    "\n",
    "We want: $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. __Forward pass:__ Compute outputs $q = x + y$, $f = qz$\n",
    "2. __Backward pass:__ Compute derivatives $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-cincinnati",
   "metadata": {
    "id": "referenced-virginia"
   },
   "source": [
    "![backprob.drawio.png](images/backprob.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-baseball",
   "metadata": {
    "id": "interesting-palestinian"
   },
   "source": [
    "$q = x +y $  &nbsp;&nbsp;&nbsp;  $\\frac{\\partial q}{\\partial x}= 1, $ &nbsp;&nbsp;&nbsp; $\\frac{\\partial q}{\\partial y}= 1$\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{\\partial f}{\\partial f}= 1 $\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$f = qz $  &nbsp;&nbsp;&nbsp;  $\\frac{\\partial f}{\\partial q}= z, $ &nbsp;&nbsp;&nbsp; $\\frac{\\partial f}{\\partial z}= q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-night",
   "metadata": {
    "id": "clear-employment"
   },
   "source": [
    "Chain rule:  $\\frac{\\partial f}{\\partial y}= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y}$, &nbsp;&nbsp;&nbsp;  $\\frac{\\partial f}{\\partial x}= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-accreditation",
   "metadata": {
    "id": "amber-gross"
   },
   "source": [
    "- The gradient computation can be automatically inferred from the symbolic expression of the forward propagation.\n",
    "- Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output.\n",
    "- Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-stamp",
   "metadata": {
    "id": "unusual-royal"
   },
   "source": [
    "Which means we do the following:\n",
    "1. Forward propagation: visit nodes in topological sort order\n",
    "    - Compute value of node given predecessors\n",
    "2. Backward propagation:\n",
    "    - initialize output gradient = 1\n",
    "    - visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors\n",
    "        $\\left\\{y_{1}, y_{2}, \\ldots y_{n}\\right\\}=\\text { successors of } x $, \n",
    "        $$\n",
    "        \\frac{\\partial z}{\\partial x}=\\sum_{i=1}^{n} \\frac{\\partial z}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial x}\n",
    "        $$\n",
    "\n",
    "If computationally done right, big $O()$ complexity of forward propagation and backward propagation is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-sullivan",
   "metadata": {
    "id": "fundamental-amendment"
   },
   "source": [
    "##  Computational graph by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-swedish",
   "metadata": {
    "id": "foster-algebra"
   },
   "source": [
    "- Draw the computational graph of the following function: $f = (x+y)z $ &nbsp;&nbsp; with &nbsp;&nbsp; $x=1, y = 3, z = -3$\n",
    "- Compute forward and backward propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19df64",
   "metadata": {},
   "source": [
    "![compgraph.JPG](images/compgraph.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1dd53",
   "metadata": {},
   "source": [
    "Forrward Pass:  <br><br>\n",
    "$q = x + y = 1 + 3 = 4$ <br><br>\n",
    "$f = q * z = 4 * (-3) = -12$\n",
    "Backward Pass: <br><br>\n",
    "$\\frac{\\partial f}{\\partial q}= z =  -3$, <br><br>\n",
    "$\\frac{\\partial f}{\\partial x}= 1 * z = -3$ <br><br>\n",
    "$\\frac{\\partial f}{\\partial y}= 1 * z = -3$ <br><br>\n",
    "$\\frac{\\partial f}{\\partial z}= q = 1 + 3 = 4$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d52716",
   "metadata": {},
   "source": [
    "##  Backpropagation with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a1456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: tensor([-3.])\n",
      "df/dy: tensor([-3.])\n",
      "df/dz: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the variables\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = (x + y) * z\n",
    "\n",
    "# Compute the gradients\n",
    "f.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"df/dx:\", x.grad)\n",
    "print(\"df/dy:\", y.grad)\n",
    "print(\"df/dz:\", z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a62d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"329pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 329.00 282.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 278)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-278 325,-278 325,4 -4,4\"/>\n",
       "<!-- 4579314960 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4579314960</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"242,-30.5 188,-30.5 188,0 242,0 242,-30.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-17\" font-family=\"monospace\" font-size=\"10.00\">f</text>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 4564544160 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4564544160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-85.75 171,-85.75 171,-66.5 259,-66.5 259,-85.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-72.25\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 4564544160&#45;&gt;4579314960 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4564544160&#45;&gt;4579314960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215,-66.18C215,-59.65 215,-50.45 215,-41.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.5,-41.78 215,-31.78 211.5,-41.78 218.5,-41.78\"/>\n",
       "</g>\n",
       "<!-- 4578909376 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4578909376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"203,-141 115,-141 115,-121.75 203,-121.75 203,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-127.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 4578909376&#45;&gt;4564544160 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4578909376&#45;&gt;4564544160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168.5,-121.34C176.51,-113.72 188.18,-102.62 197.83,-93.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.81,-96.45 204.64,-87.02 194.98,-91.38 199.81,-96.45\"/>\n",
       "</g>\n",
       "<!-- 4578914464 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4578914464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-201.88 0,-201.88 0,-182.62 100,-182.62 100,-201.88\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-188.38\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 4578914464&#45;&gt;4578909376 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4578914464&#45;&gt;4578909376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.55,-182.31C84.16,-172.8 112.26,-157.62 132.86,-146.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.15,-149.23 141.29,-141.4 130.83,-143.07 134.15,-149.23\"/>\n",
       "</g>\n",
       "<!-- 4579315536 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4579315536</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-274 23,-274 23,-243.5 77,-243.5 77,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-260.5\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-249.25\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 4579315536&#45;&gt;4578914464 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4579315536&#45;&gt;4578914464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-243.11C50,-234.13 50,-222.51 50,-212.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-213.07 50,-203.07 46.5,-213.07 53.5,-213.07\"/>\n",
       "</g>\n",
       "<!-- 4578911488 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4578911488</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-201.88 118,-201.88 118,-182.62 218,-182.62 218,-201.88\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-188.38\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 4578911488&#45;&gt;4578909376 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4578911488&#45;&gt;4578909376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.63,-182.31C165.42,-174.35 163.6,-162.44 162.04,-152.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.35,-151.76 160.38,-142.4 158.43,-152.82 165.35,-151.76\"/>\n",
       "</g>\n",
       "<!-- 4579315632 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4579315632</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"195,-274 141,-274 141,-243.5 195,-243.5 195,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-260.5\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-249.25\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 4579315632&#45;&gt;4578911488 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4579315632&#45;&gt;4578911488</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168,-243.11C168,-234.13 168,-222.51 168,-212.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.5,-213.07 168,-203.07 164.5,-213.07 171.5,-213.07\"/>\n",
       "</g>\n",
       "<!-- 4578912016 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4578912016</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"321,-141 221,-141 221,-121.75 321,-121.75 321,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-127.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 4578912016&#45;&gt;4564544160 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4578912016&#45;&gt;4564544160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.5,-121.34C253.49,-113.72 241.82,-102.62 232.17,-93.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.02,-91.38 225.36,-87.02 230.19,-96.45 235.02,-91.38\"/>\n",
       "</g>\n",
       "<!-- 4579315728 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>4579315728</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"298,-207.5 244,-207.5 244,-177 298,-177 298,-207.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\">z</text>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-182.75\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 4579315728&#45;&gt;4578912016 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4579315728&#45;&gt;4578912016</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271,-176.7C271,-169.37 271,-160.37 271,-152.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"274.5,-152.45 271,-142.45 267.5,-152.45 274.5,-152.45\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x110e7ff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: tensor([-3.])\n",
      "df/dy: tensor([-3.])\n",
      "df/dz: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the variables\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = (x + y) * z\n",
    "\n",
    "# Compute the gradients\n",
    "f.backward()\n",
    "\n",
    "# Create a dictionary that maps the tensor objects to their string representations\n",
    "names = {\"x\": x, \"y\": y, \"z\": z, \"f\": f}\n",
    "\n",
    "# Visualize the graph\n",
    "dot = make_dot(f, params=names)\n",
    "\n",
    "# Visualize the graph\n",
    "#dot = make_dot(f)\n",
    "#dot.view()\n",
    "# Display the graph inline\n",
    "display(dot)\n",
    "\n",
    "\n",
    "# Print the gradients\n",
    "print(\"df/dx:\", x.grad)\n",
    "print(\"df/dy:\", y.grad)\n",
    "print(\"df/dz:\", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-dietary",
   "metadata": {
    "id": "regulated-symphony"
   },
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-insertion",
   "metadata": {
    "id": "controlled-insured"
   },
   "source": [
    "## Definition of matrix multiplication\n",
    "\n",
    "Let A = $(a_{ij})$ be an $(m \\times n)$ matrix, and let $B= (b_{ij})$ be and $(r \\times s)$ matrix. If $n = r$, then the __product__ $AB$ is the $(m \\times s)$ matrix defined by: \n",
    "\n",
    "$$\n",
    "(A B)_{i j}=\\sum_{k=1}^{n} a_{i k} b_{k j}\n",
    "$$\n",
    "\n",
    "If $n \\neq r$, then the product $AB$ is __not defined__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-preserve",
   "metadata": {
    "id": "durable-escape"
   },
   "source": [
    "To illustrate the above formula, let's look at a visual example how to compute the matrix multiplication:\n",
    "![matrix_multiplication.png](images/matrix_multiplication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-ordinance",
   "metadata": {
    "id": "enclosed-decimal"
   },
   "source": [
    "Thus the product $AB$ is defined only when the inside dimensions of $A$ and $B$ are equal. In this case the outside dimensions, $m$ and $s$, give the size of $AB$. Furthermore, the $ij^{th}$ entry of $AB$ is the scalar product of the $i^{th}$ row of $A$ with the $j^{th}$ column of $B$.\n",
    "For instance,\n",
    "\n",
    "$$\\left[\\begin{array}{rrr}2 & 1 & -3 \\\\ -2 & 2 & 4\\end{array}\\right]\\left[\\begin{array}{rr}-1 & 2 \\\\ 0 & -3 \\\\ 2 & 1\\end{array}\\right]=\\left[\\begin{array}{cc}2(-1)+1(0)+(-3) 2 & 2(2)+1(-3)+(-3) 1 \\\\ (-2)(-1)+2(0)+4(2) & (-2) 2+2(-3)+4(1)\\end{array}\\right]=\\left[\\begin{array}{rr}-8 & -2 \\\\ 10 & -6\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-festival",
   "metadata": {
    "id": "rubber-tamil"
   },
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-reservation",
   "metadata": {
    "id": "difficult-control"
   },
   "source": [
    "Compute the matrix multiplication, $AB$, of the following two matrices:\n",
    "\n",
    "$$A = \\left[\\begin{array}{rrr}1 & 0 & -2 \\\\ 0 & 1 & 1\\end{array}\\right],  B = \\left[\\begin{array}{rr}3 & 1 \\\\ -1 & -2 \\\\ 1 & 1\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc87d70",
   "metadata": {},
   "source": [
    "$$AB  =\\left[\\begin{array}{rrr}1 & 0 & -2 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{rr}3 & 1 \\\\ -1 & -2 \\\\ 1 & 1\\end{array}\\right]=\\left[\\begin{array}{cc}1(3)+0(-1)+(-2)1 & 1(1)+0(-2)+(-2) 1 \\\\ 0(3)+1(-1)+1(1) & 0(1)+1(-2)+1(1)\\end{array}\\right]=\\left[\\begin{array}{rr}1 & -1 \\\\ 0 & -1\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d34b900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB =\n",
      "[[ 1 -1]\n",
      " [ 0 -1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrices\n",
    "A = np.array([[1, 0, -2], [0, 1, 1]])\n",
    "B = np.array([[3, 1], [-1, -2], [1, 1]])\n",
    "\n",
    "# Compute the matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "\n",
    "print(f\"AB =\\n{C}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
