{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conventional-wealth",
   "metadata": {
    "id": "acute-north"
   },
   "source": [
    "# About this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-alexandria",
   "metadata": {
    "id": "composite-poker"
   },
   "source": [
    "This `Jupyter Notebook` is the starter notebook for the first milestone of the project <b>Analyze Reports with Hugging Face</b>. It consists of the following tasks:\n",
    "\n",
    "1. Computing backpropagation by hand\n",
    "2. Computing Matrix mulitplications by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-catalyst",
   "metadata": {
    "id": "missing-coupon"
   },
   "source": [
    "# About this first milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-hardwood",
   "metadata": {
    "id": "complete-receptor"
   },
   "source": [
    "Most deep learning frameworks provide an automatic differentiation procedure to compute gradients\n",
    "based on the backpropagation algorithm. In these frameworks, all computations\n",
    "are represented as a graph and therefore also the gradient computation becomes a graph.\n",
    "Therefore, graphs in computation and backpropagation are both crucial deep learning fundamentals for the training of neural networks. \n",
    "\n",
    "Mathematical statements can be represented using computational graphs. This is comparable to a descriptive language that provides a functional description of the necessary computation in the context of deep learning models.\n",
    "\n",
    "\n",
    "Below you find an example of a simple network depicted as a computational graph.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-manitoba",
   "metadata": {
    "id": "optional-belfast"
   },
   "source": [
    "From the chain rule we have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial h_{1}}=\\frac{\\partial l}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}}\\left(=\\frac{\\partial l}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}}\\right)\n",
    "$$\n",
    "\n",
    "which can be represented as a computational graph as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-corps",
   "metadata": {
    "id": "harmful-church"
   },
   "source": [
    "![graph.jpg](images/graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-young",
   "metadata": {
    "id": "accomplished-combining"
   },
   "source": [
    "**Note:** Gradient computations are shown in black, application of the chain rule in green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-rebecca",
   "metadata": {
    "id": "beneficial-negative"
   },
   "source": [
    "Hence, we will use a computational graph to compute the derivatives of a function, for **task 1**. And since we are using a Transformer model, which uses matrix mulitplication for computing the different weight matrices, we will look into matrix multiplication in the **second task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-champagne",
   "metadata": {
    "id": "sudden-seeker"
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-traffic",
   "metadata": {
    "id": "greater-consultation"
   },
   "source": [
    "First, to make the graph shown above a bit more clear, let's look at the chain rule itself. First, we need to know that there are two different notations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-shepherd",
   "metadata": {
    "id": "convertible-category"
   },
   "source": [
    "The chain rule with __Newton__ notation: \n",
    "\n",
    "$(f \\circ g)^{\\prime}(x)=f^{\\prime}(g(x)) \\cdot g^{\\prime}(x)$ <br> <br>\n",
    "\n",
    "The chain rule with __Leibniz__ notation: \n",
    "\n",
    "$\\frac{d z}{d x}=\\frac{d z}{d y} \\cdot \\frac{d y}{d x}$\n",
    "\n",
    "We will use the Leibniz notation for our computations. \n",
    "\n",
    "For instance, if we want to compute the derivative of $(2 x+1)^{7}$, we would do the following:\n",
    "\n",
    "$\\begin{aligned} \\frac{d}{d x}(2 x+1)^{7} &=\\left[\\frac{d}{d(2 x+1)}(2 x+1)^{7}\\right] \\frac{d(2 x+1)}{d x} \\\\ &=7(2 x+1)^{6} \\cdot 2 \\\\ &=14(2 x+1)^{6} \\end{aligned}$\n",
    "\n",
    "So, basically, with the chain rule we must first take the derivative of the outer function keeping the inside function untouched. Then we multiply by the derivative of the inside function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-delhi",
   "metadata": {
    "id": "southern-disco"
   },
   "source": [
    "Now let's look how we can actually compute the backpropagation with a computational graph. The compuations consists of the following two computations:\n",
    "\n",
    "- Forward computation\n",
    "- Backward computation\n",
    "\n",
    "The process for determining the value of the mathematical expression represented by computational graphs is known as the __forward pass__. Forward pass refers to the process of sending data from variables from the left (input) to the right (output) in a forward manner.\n",
    "\n",
    "Let us consider a concrete example to demonstrate its working:\n",
    "\n",
    "$f(x, y, z)=(x+y) z$ <br>\n",
    "e.g. $\\mathrm{x}=-2, \\mathrm{y}=5, \\mathrm{z}=-4$\n",
    "\n",
    "We want: $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. __Forward pass:__ Compute outputs $q = x + y$, $f = qz$\n",
    "2. __Backward pass:__ Compute derivatives $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-cincinnati",
   "metadata": {
    "id": "referenced-virginia"
   },
   "source": [
    "![backprob.drawio.png](images/backprob.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-baseball",
   "metadata": {
    "id": "interesting-palestinian"
   },
   "source": [
    "$q = x +y $  &nbsp;&nbsp;&nbsp;  $\\frac{\\partial q}{\\partial x}= 1, $ &nbsp;&nbsp;&nbsp; $\\frac{\\partial q}{\\partial y}= 1$\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{\\partial f}{\\partial f}= 1 $\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$f = qz $  &nbsp;&nbsp;&nbsp;  $\\frac{\\partial f}{\\partial q}= z, $ &nbsp;&nbsp;&nbsp; $\\frac{\\partial f}{\\partial z}= q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-night",
   "metadata": {
    "id": "clear-employment"
   },
   "source": [
    "Chain rule:  $\\frac{\\partial f}{\\partial y}= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y}$, &nbsp;&nbsp;&nbsp;  $\\frac{\\partial f}{\\partial x}= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-accreditation",
   "metadata": {
    "id": "amber-gross"
   },
   "source": [
    "- The gradient computation can be automatically inferred from the symbolic expression of the forward propagation.\n",
    "- Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output.\n",
    "- Modern DL frameworks (Tensorflow, PyTorch, etc.) do backpropagation for you but mainly leave layer/node writer to hand-calculate the local derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-stamp",
   "metadata": {
    "id": "unusual-royal"
   },
   "source": [
    "Which means we do the following:\n",
    "1. Forward propagation: visit nodes in topological sort order\n",
    "    - Compute value of node given predecessors\n",
    "2. Backward propagation:\n",
    "    - initialize output gradient = 1\n",
    "    - visit nodes in reverse order: Compute gradient wrt each node using gradient wrt successors\n",
    "        $\\left\\{y_{1}, y_{2}, \\ldots y_{n}\\right\\}=\\text { successors of } x $, \n",
    "        $$\n",
    "        \\frac{\\partial z}{\\partial x}=\\sum_{i=1}^{n} \\frac{\\partial z}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial x}\n",
    "        $$\n",
    "\n",
    "If computationally done right, big $O()$ complexity of forward propagation and backward propagation is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-sullivan",
   "metadata": {
    "id": "fundamental-amendment"
   },
   "source": [
    "<font color='red'> <h1> TODO 1</h1> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-swedish",
   "metadata": {
    "id": "foster-algebra"
   },
   "source": [
    "- Draw the computational graph of the following function: $f = (x+y)z $ &nbsp;&nbsp; with &nbsp;&nbsp; $x=1, y = 3, z = -3$\n",
    "- Compute forward and backward propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19df64",
   "metadata": {},
   "source": [
    "![compgraph.JPG](images/compgraph.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1dd53",
   "metadata": {},
   "source": [
    "Forrward Pass:  <br><br>\n",
    "$q = x + y = 1 + 3 = 4$ <br><br>\n",
    "$f = q * z = 4 * (-3) = -12$\n",
    "\n",
    "Backward Pass: <br><br>\n",
    "$\\frac{\\partial f}{\\partial q}= z =  -3$, <br><br>\n",
    "$\\frac{\\partial f}{\\partial x}= 1 * z = -3$ <br><br>\n",
    "$\\frac{\\partial f}{\\partial y}= 1 * z = -3$ <br><br>\n",
    "$\\frac{\\partial f}{\\partial z}= q = 1 + 3 = 4$ <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a1456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: tensor([-3.])\n",
      "df/dy: tensor([-3.])\n",
      "df/dz: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the variables\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = (x + y) * z\n",
    "\n",
    "# Compute the gradients\n",
    "f.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"df/dx:\", x.grad)\n",
    "print(\"df/dy:\", y.grad)\n",
    "print(\"df/dz:\", z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a62d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: tensor([-3.])\n",
      "df/dy: tensor([-3.])\n",
      "df/dz: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define the variables\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = (x + y) * z\n",
    "\n",
    "# Compute the gradients\n",
    "f.backward()\n",
    "\n",
    "# Visualize the graph\n",
    "dot = make_dot(f)\n",
    "dot.view()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"df/dx:\", x.grad)\n",
    "print(\"df/dy:\", y.grad)\n",
    "print(\"df/dz:\", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-dietary",
   "metadata": {
    "id": "regulated-symphony"
   },
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-insertion",
   "metadata": {
    "id": "controlled-insured"
   },
   "source": [
    "## Definition of matrix multiplication\n",
    "\n",
    "Let A = $(a_{ij})$ be an $(m \\times n)$ matrix, and let $B= (b_{ij})$ be and $(r \\times s)$ matrix. If $n = r$, then the __product__ $AB$ is the $(m \\times s)$ matrix defined by: \n",
    "\n",
    "$$\n",
    "(A B)_{i j}=\\sum_{k=1}^{n} a_{i k} b_{k j}\n",
    "$$\n",
    "\n",
    "If $n \\neq r$, then the product $AB$ is __not defined__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-preserve",
   "metadata": {
    "id": "durable-escape"
   },
   "source": [
    "To illustrate the above formula, let's look at a visual example how to compute the matrix multiplication:\n",
    "![matrix_multiplication.png](images/matrix_multiplication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-support",
   "metadata": {},
   "source": [
    "**Image credit:** - Johnson, Riess and Arnold [Introduction to Linear Algebra](https://www.pearson.ch/HigherEducation/Pearson/EAN/9780201658590/Introduction-to-Linear-Algebra), Pearson, 2001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-ordinance",
   "metadata": {
    "id": "enclosed-decimal"
   },
   "source": [
    "Thus the product $AB$ is defined only when the inside dimensions of $A$ and $B$ are equal. In this case the outside dimensions, $m$ and $s$, give the size of $AB$. Furthermore, the $ij^{th}$ entry of $AB$ is the scalar product of the $i^{th}$ row of $A$ with the $j^{th}$ column of $B$.\n",
    "For instance,\n",
    "\n",
    "$$\\left[\\begin{array}{rrr}2 & 1 & -3 \\\\ -2 & 2 & 4\\end{array}\\right]\\left[\\begin{array}{rr}-1 & 2 \\\\ 0 & -3 \\\\ 2 & 1\\end{array}\\right]=\\left[\\begin{array}{cc}2(-1)+1(0)+(-3) 2 & 2(2)+1(-3)+(-3) 1 \\\\ (-2)(-1)+2(0)+4(2) & (-2) 2+2(-3)+4(1)\\end{array}\\right]=\\left[\\begin{array}{rr}-8 & -2 \\\\ 10 & -6\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-festival",
   "metadata": {
    "id": "rubber-tamil"
   },
   "source": [
    "<font color='red'> <h1> TODO 2</h1> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-reservation",
   "metadata": {
    "id": "difficult-control"
   },
   "source": [
    "Now it's your turn to compute the matrix multiplication, $AB$, of the following two matrices:\n",
    "\n",
    "$$A = \\left[\\begin{array}{rrr}1 & 0 & -2 \\\\ 0 & 1 & 1\\end{array}\\right],  B = \\left[\\begin{array}{rr}3 & 1 \\\\ -1 & -2 \\\\ 1 & 1\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc87d70",
   "metadata": {},
   "source": [
    "$$AB  =\\left[\\begin{array}{rrr}1 & 0 & -2 \\\\ 0 & 1 & 1\\end{array}\\right]\\left[\\begin{array}{rr}3 & 1 \\\\ -1 & -2 \\\\ 1 & 1\\end{array}\\right]=\\left[\\begin{array}{cc}1(3)+0(-1)+(-2)1 & 1(1)+0(-2)+(-2) 1 \\\\ 0(3)+1(-1)+1(1) & 0(1)+1(-2)+1(1)\\end{array}\\right]=\\left[\\begin{array}{rr}1 & -1 \\\\ 0 & -1\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d34b900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB =\n",
      "[[ 1 -1]\n",
      " [ 0 -1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrices\n",
    "A = np.array([[1, 0, -2], [0, 1, 1]])\n",
    "B = np.array([[3, 1], [-1, -2], [1, 1]])\n",
    "\n",
    "# Compute the matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "\n",
    "print(f\"AB =\\n{C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-dancing",
   "metadata": {
    "id": "valued-daughter"
   },
   "source": [
    "__Ressources:__\n",
    "- Strang, Gilbert [Linear Algebra and Learning from Data](https://math.mit.edu/~gs/learningfromdata/), Wellesley-Cambridge Press, 2019. \n",
    "- Strang, Gilbert [Introduction to Linear Algebra, Fifth Edition](https://math.mit.edu/~gs/linearalgebra/), Wellesley-Cambridge Press, 2016.\n",
    "- Johnson, Riess and Arnold [Introduction to Linear Algebra](https://www.pearson.ch/HigherEducation/Pearson/EAN/9780201658590/Introduction-to-Linear-Algebra), Pearson, 2001\n",
    "- Standford University [CS 224](cs224n-2019-lecture04-backprop.pdf) lecture on back propagation\n",
    "- ETH Zurich [lectures on deep learning](http://da.inf.ethz.ch/teaching/2021/DeepLearning/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
